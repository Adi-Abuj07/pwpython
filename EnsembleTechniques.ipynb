{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "dd4ab09f-774c-45f4-b805-6d625ad1f2f3",
      "cell_type": "code",
      "source": "## Ensemble Techniques in Machine Learning (Beginner-Friendly)\n\n### Q1. What is an ensemble technique in machine learning?\nEnsemble techniques combine multiple models to improve accuracy and reduce errors. Instead of relying on a single model, these methods use a group of models to make better predictions.\n\n### Q2. Why are ensemble techniques used in machine learning?\nThey are used to increase accuracy, reduce overfitting, and make models more robust by combining different perspectives from multiple models.\n\n### Q3. What is bagging?\nBagging (Bootstrap Aggregating) is an ensemble technique where multiple models are trained on different random subsets of the data, and their predictions are averaged (or voted) to improve stability and accuracy.\n\n### Q4. What is boosting?\nBoosting is an ensemble technique that builds models sequentially, where each new model learns from the mistakes of the previous ones, making the final prediction stronger.\n\n### Q5. What are the benefits of using ensemble techniques?\n- Higher accuracy\n- Reduced overfitting\n- More stable predictions\n- Works well with different data distributions\n\n### Q6. Are ensemble techniques always better than individual models?\nNot always. They may increase complexity and require more computation, but in most cases, they provide better accuracy compared to a single model.\n\n### Q7. How is the confidence interval calculated using bootstrap?\nBootstrap calculates confidence intervals by repeatedly sampling data with replacement and computing the statistic of interest (like the mean). The interval is derived from percentiles of these repeated computations.\n\n### Q8. How does bootstrap work, and what are the steps involved?\n1. Take multiple random samples from the dataset with replacement.\n2. Calculate the desired statistic (mean, median, etc.) for each sample.\n3. Repeat this process many times (e.g., 1000 times).\n4. Compute the confidence interval using the percentiles of the results.\n\n### Q9. Bootstrap example: Estimating the 95% confidence interval for mean height\n```python\nimport numpy as np\n\n# Given sample data\nnp.random.seed(42)\ndata = np.random.normal(loc=15, scale=2, size=50)  # Mean=15m, Std=2m\n\n# Bootstrap sampling\ndef bootstrap_ci(data, num_samples=1000, confidence_level=95):\n    sample_means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(num_samples)]\n    lower_bound = np.percentile(sample_means, (100 - confidence_level) / 2)\n    upper_bound = np.percentile(sample_means, 100 - (100 - confidence_level) / 2)\n    return lower_bound, upper_bound\n\nci_lower, ci_upper = bootstrap_ci(data)\nprint(f\"95% Confidence Interval for mean height: ({ci_lower:.2f}, {ci_upper:.2f}) meters\")\n```\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}