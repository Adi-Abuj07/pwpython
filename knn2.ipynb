{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "dd4ab09f-774c-45f4-b805-6d625ad1f2f3",
      "cell_type": "code",
      "source": "### K-Nearest Neighbors (KNN) Algorithm - Beginner Friendly Guide\n\n#### Q1. What is the KNN algorithm?\nK-Nearest Neighbors (KNN) is a simple, non-parametric machine learning algorithm used for classification and regression. It works by finding the `k` closest data points to a given input and making predictions based on the majority class (for classification) or the average (for regression).\n\n#### Q2. How do you choose the value of K in KNN?\nThe choice of `k` affects model performance. A small `k` (e.g., 1) makes the model sensitive to noise, while a large `k` (e.g., 100) can lead to oversmoothing. A good approach is to use cross-validation to find the best `k` value.\n\n#### Q3. What is the difference between KNN classifier and KNN regressor?\n- **KNN Classifier**: Predicts class labels by taking a majority vote from the `k` nearest neighbors.\n- **KNN Regressor**: Predicts numerical values by averaging the `k` nearest neighbors' values.\n\n#### Q4. How do you measure the performance of KNN?\n- **For classification**: Accuracy, precision, recall, F1-score, and confusion matrix.\n- **For regression**: Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared value.\n\n#### Q5. What is the curse of dimensionality in KNN?\nKNN relies on distance calculations, and as dimensions increase, data points become sparse, making it harder to find meaningful neighbors. This can degrade performance.\n\n#### Q6. How do you handle missing values in KNN?\n- Use imputation techniques like filling missing values with the mean (for numerical data) or the most frequent category (for categorical data).\n- Alternatively, use KNN imputation, which finds similar data points and fills in missing values based on their neighbors.\n\n#### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n- **KNN Classifier**: Works well for problems where decision boundaries are not linear and requires little assumption about data distribution.\n- **KNN Regressor**: Useful for continuous-valued prediction tasks but can struggle with high-dimensional or noisy data.\n\n#### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n- **Strengths**:\n  - Simple to implement and interpret.\n  - Works well with small datasets.\n- **Weaknesses**:\n  - Computationally expensive for large datasets.\n  - Sensitive to irrelevant features and unscaled data.\n  - Requires careful choice of `k`.\n- **Solutions**:\n  - Use feature selection and dimensionality reduction techniques.\n  - Normalize features to ensure equal weightage.\n  - Use KD-trees or Ball-trees for faster nearest-neighbor search.\n\n#### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n- **Euclidean Distance**: Measures the straight-line distance between two points. Suitable for continuous data.\n- **Manhattan Distance**: Measures the sum of absolute differences along each dimension. Works well when features have different scales or in grid-based data.\n\n#### Q10. What is the role of feature scaling in KNN?\nKNN relies on distance calculations, so unscaled features can dominate the predictions. Standardizing or normalizing the data ensures all features contribute equally, improving model performance.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}