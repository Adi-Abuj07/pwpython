{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "dd4ab09f-774c-45f4-b805-6d625ad1f2f3",
      "cell_type": "code",
      "source": "## Boosting in Machine Learning\n\n### Q1. What is boosting in machine learning?\nBoosting is an ensemble learning technique that combines multiple weak learners to create a strong learner. It sequentially trains models, giving more importance to misclassified samples.\n\n### Q2. What are the advantages and limitations of using boosting techniques?\n**Advantages:**\n- Improves accuracy by reducing bias and variance.\n- Works well with weak learners.\n- Reduces overfitting when tuned properly.\n\n**Limitations:**\n- Sensitive to noisy data and outliers.\n- Can be computationally expensive.\n\n### Q3. Explain how boosting works.\nBoosting works by training models sequentially, where each new model corrects errors made by the previous one. Misclassified samples are given higher weights to focus on difficult cases.\n\n### Q4. What are the different types of boosting algorithms?\n- AdaBoost (Adaptive Boosting)\n- Gradient Boosting\n- XGBoost\n- LightGBM\n- CatBoost\n\n### Q5. What are some common parameters in boosting algorithms?\n- Number of estimators (weak learners)\n- Learning rate\n- Maximum depth of trees\n- Subsample ratio\n\n### Q6. How do boosting algorithms combine weak learners to create a strong learner?\nBoosting assigns weights to weak learners and aggregates their predictions using a weighted sum, improving accuracy by focusing on hard-to-classify samples.\n\n### Q7. Explain the concept of AdaBoost algorithm and its working.\nAdaBoost assigns higher weights to misclassified samples and trains weak classifiers iteratively. Each classifierâ€™s contribution is weighted based on its accuracy.\n\n### Q8. What is the loss function used in AdaBoost algorithm?\nAdaBoost minimizes the exponential loss function, which penalizes misclassified samples heavily.\n\n### Q9. How does the AdaBoost algorithm update the weights of misclassified samples?\nAfter each iteration, misclassified samples get increased weights, making them more influential in the next model training step.\n\n### Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?\nIncreasing the number of estimators improves performance initially but may lead to overfitting if the number is too high.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}