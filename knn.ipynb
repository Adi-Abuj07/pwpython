{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "dd4ab09f-774c-45f4-b805-6d625ad1f2f3",
      "cell_type": "code",
      "source": "## Beginner-Friendly Machine Learning Pipelines\n\n### Q1: Feature Engineering Pipeline - Handling Missing Data & Feature Selection\n\n#### Steps:\n1. **Select Important Features**: Identify key features that contribute most to predictions.\n2. **Prepare Numerical Data**:\n   - Fill missing values with the mean.\n   - Standardize numerical data for better performance.\n3. **Prepare Categorical Data**:\n   - Fill missing values with the most common category.\n   - Convert categories into numerical form using one-hot encoding.\n4. **Combine Everything**: Use `ColumnTransformer` to apply transformations to both numerical and categorical features.\n5. **Train a Model**: Use `RandomForestClassifier` to make predictions.\n6. **Evaluate the Model**: Measure accuracy using test data.\n\n#### Code:\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load sample dataset\ndata = pd.read_csv(\"data.csv\")\nX = data.drop(\"target\", axis=1)\ny = data[\"target\"]\n\n# Identify numerical and categorical features\nnum_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\ncat_features = X.select_dtypes(include=[\"object\"]).columns\n\n# Create pipelines for processing\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine both pipelines\npreprocessor = ColumnTransformer([\n    ('num', num_pipeline, num_features),\n    ('cat', cat_pipeline, cat_features)\n])\n\n# Final pipeline including the model\nmodel_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n])\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the model\nmodel_pipeline.fit(X_train, y_train)\n\n# Make predictions and evaluate accuracy\ny_pred = model_pipeline.predict(X_test)\nprint(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n```\n\n### Q2: Combining Models with a Voting Classifier\n#### Steps:\n1. Train two different models: `RandomForestClassifier` and `LogisticRegression`.\n2. Combine both models using `VotingClassifier`.\n3. Test the accuracy on a dataset.\n\n#### Code:\n```python\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define two models\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nlr = LogisticRegression(max_iter=200)\n\n# Combine them in a voting classifier\nvoting_clf = VotingClassifier(estimators=[('rf', rf), ('lr', lr)], voting='hard')\nvoting_clf.fit(X_train, y_train)\n\n# Evaluate performance\nprint(\"Voting Classifier Accuracy:\", voting_clf.score(X_test, y_test))\n```\n\n### Summary:\n- We built a pipeline that automates feature selection, data processing, and model training.\n- We used two different classifiers and combined them using a voting mechanism to improve accuracy.\n- This approach ensures robustness and reduces bias in predictions.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}