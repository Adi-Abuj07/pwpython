{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "dd4ab09f-774c-45f4-b805-6d625ad1f2f3",
      "cell_type": "code",
      "source": "## Gradient Boosting in Machine Learning (Beginner-Friendly)\n\n### Q1. What is Gradient Boosting Regression?\nGradient Boosting Regression is a technique used to make better predictions by combining multiple simple models (weak learners). It works step by step, where each new model tries to fix the mistakes of the previous one. The final result is a strong prediction model.\n\n### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy.\n```python\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Generate a small dataset\nnp.random.seed(42)\nX = np.linspace(0, 10, 100).reshape(-1, 1)\ny = 3 * X.squeeze() + np.sin(X.squeeze()) + np.random.randn(100) * 0.5\n\n# Define a simple Gradient Boosting Regressor\nclass SimpleGBR:\n    def __init__(self, n_estimators=50, learning_rate=0.1):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.models = []\n        self.bias = None\n    \n    def fit(self, X, y):\n        self.bias = np.mean(y)  # Initial prediction is the mean\n        residuals = y - self.bias  # Errors\n        \n        for _ in range(self.n_estimators):\n            model = np.poly1d(np.polyfit(X.squeeze(), residuals, 1))  # Fit a simple model\n            self.models.append(model)\n            residuals -= self.learning_rate * model(X.squeeze())  # Update residuals\n    \n    def predict(self, X):\n        y_pred = np.full(X.shape[0], self.bias)\n        for model in self.models:\n            y_pred += self.learning_rate * model(X.squeeze())\n        return y_pred\n\n# Train model\ngbr = SimpleGBR(n_estimators=50, learning_rate=0.1)\ngbr.fit(X, y)\n\n# Predictions\ny_pred = gbr.predict(X)\n\n# Evaluate model\nprint(\"Mean Squared Error:\", mean_squared_error(y, y_pred))\nprint(\"R-squared Score:\", r2_score(y, y_pred))\n```\n\n### Q3. Experiment with different hyperparameters to improve performance.\n```python\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [1, 3, 5]\n}\n\ngbr = GradientBoostingRegressor()\ngrid_search = GridSearchCV(gbr, param_grid, cv=5, scoring='r2')\ngrid_search.fit(X, y)\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best R-squared Score:\", grid_search.best_score_)\n```\n\n### Q4. What is a weak learner in Gradient Boosting?\nA weak learner is a simple model that makes slightly better predictions than random guessing. Gradient Boosting builds an ensemble by improving these weak learners over time.\n\n### Q5. What is the intuition behind the Gradient Boosting algorithm?\nThe idea is to keep improving the model by focusing on errors. Each new model learns from the mistakes of previous models to make better predictions.\n\n### Q6. How does Gradient Boosting build an ensemble of weak learners?\n1. Start with an initial prediction (e.g., the average of all values).\n2. Calculate the errors (residuals).\n3. Train a simple model (weak learner) on these errors.\n4. Use this weak learner to update predictions.\n5. Repeat the process multiple times to improve accuracy.\n\n### Q7. Steps to understand the Gradient Boosting algorithm:\n1. **Start with an initial guess** (e.g., the mean of the target variable).\n2. **Compute the residuals** (actual value - predicted value).\n3. **Train a weak model** on these residuals.\n4. **Update predictions** using this weak model.\n5. **Repeat** until the model is accurate enough or we reach the set number of iterations.\n\nThis process helps the model get better with each step, leading to strong final predictions!\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}