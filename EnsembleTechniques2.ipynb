{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "dd4ab09f-774c-45f4-b805-6d625ad1f2f3",
      "cell_type": "code",
      "source": "## Ensemble Techniques in Machine Learning (Beginner-Friendly)\n\n### Q1. How does bagging reduce overfitting in decision trees?\nBagging reduces overfitting by training multiple decision trees on different random subsets of the data and averaging their predictions. This helps in smoothing out noise and reducing variance, leading to a more generalizable model.\n\n### Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n**Advantages:**\n- Decision trees: High variance models benefit the most from bagging, leading to more stable predictions.\n- Linear models: Less useful in bagging since they have lower variance.\n- Neural networks: Can improve performance but computationally expensive.\n\n**Disadvantages:**\n- Some models may not benefit much from bagging (e.g., low-variance models like linear regression).\n- Requires more computational power due to training multiple models.\n\n### Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n- **High variance models (e.g., decision trees)**: Bagging helps reduce variance, making the model more stable.\n- **Low variance models (e.g., linear regression)**: Bagging has little effect as they already have low variance.\n- **Tradeoff:** Choosing a weak learner with high bias may not yield significant improvements, while selecting a high-variance learner can greatly benefit from bagging.\n\n### Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\nYes, bagging can be used for both:\n- **Classification:** Uses majority voting from multiple models.\n- **Regression:** Uses averaging of predictions from multiple models to improve stability and reduce overfitting.\n\n### Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\nThe ensemble size (number of models) impacts performance:\n- **Too few models:** Does not fully reduce variance.\n- **Too many models:** Increases computation time with diminishing returns.\n- **Optimal range:** Typically 50â€“100 models work well, but this depends on the dataset and computational resources.\n\n### Q6. Can you provide an example of a real-world application of bagging in machine learning?\n**Example: Fraud Detection**\nBagging is used in fraud detection to improve the accuracy of classifiers. By training multiple decision trees on different samples of transaction data, bagging reduces the risk of overfitting and provides a more reliable fraud prediction model.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}